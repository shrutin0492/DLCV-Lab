{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkpOFti+r11gTYiZEs4mOW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrutin0492/Deep-Learning-and-Computer-Vision-Lab/blob/main/design_single_layer_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Designing a single layer neural network using various activation functions:**\n",
        "- *Tanh*\n",
        "- *ReLU*\n",
        "- *Leaky ReLU*\n",
        "- *Sigmoid*"
      ],
      "metadata": {
        "id": "ASsgCZ-E9Ns8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tanh"
      ],
      "metadata": {
        "id": "8WzcwtY46mVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import exp, array, random, dot, tanh\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        random.seed(1)\n",
        "        self.weight_matrix = 2 * random.random((3, 1)) - 1\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return tanh(x)\n",
        "\n",
        "    def tanh_derivative(self, x):\n",
        "        return 1.0 - tanh(x)**2\n",
        "\n",
        "    def forward_propagation(self, inputs):\n",
        "        return self.tanh(dot(inputs, self.weight_matrix))\n",
        "\n",
        "    def train(self, train_inputs, train_outputs, num_train_iterations):\n",
        "        for iteration in range(num_train_iterations):\n",
        "            output = self.forward_propagation(train_inputs)\n",
        "            error = train_outputs - output\n",
        "            adjustment = dot(train_inputs.T, error * self.tanh_derivative(output))\n",
        "            self.weight_matrix += adjustment\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    neural_network = NeuralNetwork()\n",
        "    print('Random weights at the start of training')\n",
        "    print(neural_network.weight_matrix)\n",
        "    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
        "    train_outputs = array([[0, 1, 1, 0]]).T\n",
        "    neural_network.train(train_inputs, train_ouâ€“tputs, 10000)\n",
        "    print('New weights after training')\n",
        "    print(neural_network.weight_matrix)\n",
        "    print(\"Testing network on new examples->\")\n",
        "    print(neural_network.forward_propagation(array([1, 0, 0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoMJIMzq5i5m",
        "outputId": "bb428218-1ee4-4014-8b00-4b6f49cd31c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random weights at the start of training\n",
            "[[-0.16595599]\n",
            " [ 0.44064899]\n",
            " [-0.99977125]]\n",
            "New weights after training\n",
            "[[5.39428067]\n",
            " [0.19482422]\n",
            " [0.34317086]]\n",
            "Testing network on new examples->\n",
            "[0.99995873]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReLU"
      ],
      "metadata": {
        "id": "r10Gz9dL5phc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array, random, dot\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        random.seed(1)\n",
        "        # He initialization for weights\n",
        "        self.weight_matrix = random.randn(3, 1) * (2 / 3)  # 2/3 is the fan-in for this layer\n",
        "\n",
        "    def relu(self, x):\n",
        "        # ReLU activation function\n",
        "        return x * (x > 0)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        # Derivative of ReLU activation function\n",
        "        return 1. * (x > 0)\n",
        "\n",
        "    def forward_propagation(self, inputs):\n",
        "        # Forward propagation using ReLU activation function\n",
        "        return self.relu(dot(inputs, self.weight_matrix))\n",
        "\n",
        "    def train(self, train_inputs, train_outputs, num_train_iterations, learning_rate=0.01):\n",
        "        # Training the neural network\n",
        "        for iteration in range(num_train_iterations):\n",
        "            output = self.forward_propagation(train_inputs)\n",
        "            error = train_outputs - output\n",
        "            # Adjusting weights using gradient descent with learning rate\n",
        "            adjustment = learning_rate * dot(train_inputs.T, error * self.relu_derivative(output))\n",
        "            self.weight_matrix += adjustment\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    neural_network = NeuralNetwork()\n",
        "    print('Random weights at the start of training')\n",
        "    print(neural_network.weight_matrix)\n",
        "    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
        "    train_outputs = array([[0, 1, 1, 0]]).T\n",
        "    neural_network.train(train_inputs, train_outputs, 10000, learning_rate=0.1)  # Adjusted learning rate\n",
        "    print('New weights after training')\n",
        "    print(neural_network.weight_matrix)\n",
        "    print(\"Testing network on new examples->\")\n",
        "    print(neural_network.forward_propagation(array([1, 0, 0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFEXSbZT7wYK",
        "outputId": "848401e4-fdd6-46cb-e093-75e0eaf3b275"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random weights at the start of training\n",
            "[[ 1.08289691]\n",
            " [-0.40783761]\n",
            " [-0.3521145 ]]\n",
            "New weights after training\n",
            "[[ 1.21750571e+00]\n",
            " [-1.01856302e-16]\n",
            " [-2.17505705e-01]]\n",
            "Testing network on new examples->\n",
            "[1.21750571]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leaky ReLU"
      ],
      "metadata": {
        "id": "JI2T_1fz7uWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import exp, array, random, dot\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        random.seed(1)\n",
        "        # Xavier initialization for weights\n",
        "        self.weight_matrix = random.randn(3, 1) * (2 / 3)  # 2/3 is the fan-in for this layer\n",
        "\n",
        "    def leaky_relu(self, x):\n",
        "        # Leaky ReLU activation function\n",
        "        return x * (x > 0) + 0.01 * x * (x <= 0)  # 0.01 is the slope for negative inputs\n",
        "\n",
        "    def leaky_relu_derivative(self, x):\n",
        "        # Derivative of Leaky ReLU activation function\n",
        "        return 1. * (x > 0) + 0.01 * (x <= 0)  # 0.01 is the slope for negative inputs\n",
        "\n",
        "    def forward_propagation(self, inputs):\n",
        "        # Forward propagation using Leaky ReLU activation function\n",
        "        return self.leaky_relu(dot(inputs, self.weight_matrix))\n",
        "\n",
        "    def train(self, train_inputs, train_outputs, num_train_iterations):\n",
        "        # Training the neural network\n",
        "        for iteration in range(num_train_iterations):\n",
        "            output = self.forward_propagation(train_inputs)\n",
        "            error = train_outputs - output\n",
        "            # Adjusting weights using gradient descent\n",
        "            adjustment = dot(train_inputs.T, error * self.leaky_relu_derivative(output))\n",
        "            self.weight_matrix += adjustment\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    neural_network = NeuralNetwork()\n",
        "    print('Random weights at the start of training')\n",
        "    print(neural_network.weight_matrix)\n",
        "    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
        "    train_outputs = array([[0, 1, 1, 0]]).T\n",
        "    neural_network.train(train_inputs, train_outputs, 10000)\n",
        "    print('New weights after training')\n",
        "    print(neural_network.weight_matrix)\n",
        "    print(\"Testing network on new examples->\")\n",
        "    print(neural_network.forward_propagation(array([1, 0, 0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GIdLgg17f8M",
        "outputId": "2fba3933-0ac4-4e11-e226-54df4487a627"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random weights at the start of training\n",
            "[[ 1.08289691]\n",
            " [-0.40783761]\n",
            " [-0.3521145 ]]\n",
            "New weights after training\n",
            "[[ 2.00223261]\n",
            " [-0.96519995]\n",
            " [-0.00686515]]\n",
            "Testing network on new examples->\n",
            "[2.00223261]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sigmoid"
      ],
      "metadata": {
        "id": "Ke2r5CmJ6joq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array, random, dot, exp\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        random.seed(1)\n",
        "        # Xavier initialization for weights\n",
        "        self.weight_matrix = random.randn(3, 1) * (2 / 3)  # 2/3 is the fan-in for this layer\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # Sigmoid activation function\n",
        "        return 1 / (1 + exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        # Derivative of sigmoid activation function\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward_propagation(self, inputs):\n",
        "        # Forward propagation using sigmoid activation function\n",
        "        return self.sigmoid(dot(inputs, self.weight_matrix))\n",
        "\n",
        "    def train(self, train_inputs, train_outputs, num_train_iterations, learning_rate=0.01):\n",
        "        # Training the neural network\n",
        "        for iteration in range(num_train_iterations):\n",
        "            output = self.forward_propagation(train_inputs)\n",
        "            error = train_outputs - output\n",
        "            # Adjusting weights using gradient descent with learning rate\n",
        "            adjustment = learning_rate * dot(train_inputs.T, error * self.sigmoid_derivative(output))\n",
        "            self.weight_matrix += adjustment\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    neural_network = NeuralNetwork()\n",
        "    print('Random weights at the start of training')\n",
        "    print(neural_network.weight_matrix)\n",
        "    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
        "    train_outputs = array([[0, 1, 1, 0]]).T\n",
        "    neural_network.train(train_inputs, train_outputs, 10000, learning_rate=0.1)  # Adjusted learning rate\n",
        "    print('New weights after training')\n",
        "    print(neural_network.weight_matrix)\n",
        "    print(\"Testing network on new examples->\")\n",
        "    print(neural_network.forward_propagation(array([1, 0, 0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGBmJYFH8wsV",
        "outputId": "61a83d90-23f8-42b1-a358-b9f17a8dd9b9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random weights at the start of training\n",
            "[[ 1.08289691]\n",
            " [-0.40783761]\n",
            " [-0.3521145 ]]\n",
            "New weights after training\n",
            "[[ 7.26526317]\n",
            " [-0.22329088]\n",
            " [-3.41398404]]\n",
            "Testing network on new examples->\n",
            "[0.99930107]\n"
          ]
        }
      ]
    }
  ]
}